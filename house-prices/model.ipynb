{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running and evaluating the model\n",
    "\n",
    "We use Tensorflow to run the simple linear regression model as we can easily use its dataset objects to create the data pipeline and we have ready made optimisers to use too.  Also, by writing the model in Tensorflow we could easily change it to a more complicated neural network model at a later stage.\n",
    "\n",
    "We have used scikit-optimize to optimize the hyperparameters of the model using the 2014 validation dataset and then train the model on the all of the training data (including the validation dataset).  We obtain a mean error of Â£640 on the test (2015) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# for hyperparameter search\n",
    "# pip install scikit-optimize\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "path_train = 'train_shuf.csv'\n",
    "path_val = 'val.csv'\n",
    "path_test = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_xy(*tup):\n",
    "    x = tf.stack(tup[1:], axis=0)\n",
    "    y = tf.squeeze(tup[0])\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 ** 12\n",
    "shuffle_size = 10 ** 6\n",
    "\n",
    "\n",
    "def construct_datasets(num_epochs, val):\n",
    "    # Construct the datasets\n",
    "    # val = True: use the 2014 data for the test dataset\n",
    "    # val = False: use 2015 data for test data set and val & train data for train dataset\n",
    "    if val:\n",
    "        filenames_train = [path_train]\n",
    "        filenames_test = path_val\n",
    "    else:\n",
    "        filenames_train = [path_train, path_val]\n",
    "        filenames_test = path_test\n",
    "\n",
    "    prefetch = 10\n",
    "    defaults = [tf.float32] * 8\n",
    "\n",
    "    # make sure the dataset is on the CPU to leave the GPU for training the model\n",
    "    # although this model is so simple it's quicker to leave the training on the CPU too\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.name_scope('dataset_train'):\n",
    "            dataset_train = tf.data.experimental.CsvDataset(\n",
    "                filenames_train, defaults)\n",
    "            dataset_train = dataset_train.apply(\n",
    "                tf.data.experimental.shuffle_and_repeat(shuffle_size, count=num_epochs))\n",
    "            dataset_train = dataset_train.map(\n",
    "                map_to_xy).batch(batch_size).prefetch(prefetch)\n",
    "            next_element_train = dataset_train.make_one_shot_iterator().get_next()\n",
    "\n",
    "        with tf.variable_scope('dataset_test'):\n",
    "            dataset_test = tf.data.experimental.CsvDataset(\n",
    "                filenames_test, defaults)\n",
    "            dataset_test = dataset_test.map(map_to_xy).batch(\n",
    "                batch_size).prefetch(prefetch)\n",
    "            next_element_test = dataset_test.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return next_element_train, next_element_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs=1, val=True, learning_rate=20000, reg=0.006, alpha=1.0):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # construct the datasets\n",
    "    next_element_train, next_element_test = construct_datasets(num_epochs, val)\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 7])\n",
    "    y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "    # create simple linear model with elastic regularization\n",
    "    scores = tf.layers.dense(\n",
    "        x, 1, kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(alpha, 1-alpha))\n",
    "    scores = tf.squeeze(scores)\n",
    "\n",
    "    # use l2 loss\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.square(scores - y)\n",
    "        loss_mean = tf.reduce_mean(loss)\n",
    "        loss_reg = reg * tf.losses.get_regularization_loss()\n",
    "        loss_total = loss_mean + loss_reg\n",
    "\n",
    "    # tensorboard logging\n",
    "    tf.summary.scalar('loss_reg', loss_reg)\n",
    "    tf.summary.scalar('loss', loss_mean)\n",
    "    tf.summary.scalar('loss_total', loss_total)\n",
    "    global_step = tf.Variable(1, trainable=False, name='global_step')\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_total, global_step=global_step)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_writer = tf.summary.FileWriter('train')\n",
    "\n",
    "        # train\n",
    "        while True:\n",
    "            try:\n",
    "                (x_np, y_np) = sess.run(next_element_train)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            feed_dict = {x: x_np, y: y_np}\n",
    "            summary, _ = sess.run([merged, train_op], feed_dict=feed_dict)\n",
    "            train_writer.add_summary(\n",
    "                summary, tf.train.global_step(sess, global_step))\n",
    "\n",
    "        # test\n",
    "        scores_list = []\n",
    "        loss_list = []\n",
    "        while True:\n",
    "            try:\n",
    "                (x_np, y_np) = sess.run(next_element_test)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            feed_dict = {x: x_np, y: y_np}\n",
    "            test_loss_tmp, test_scores_tmp = sess.run(\n",
    "                [loss, scores], feed_dict=feed_dict)\n",
    "            scores_list.append(test_scores_tmp)\n",
    "            loss_list.append(test_loss_tmp)\n",
    "\n",
    "        test_scores = np.concatenate(scores_list)\n",
    "        test_loss = np.concatenate(loss_list)\n",
    "        test_loss_av = np.sqrt(np.sum(test_loss)) / len(test_loss)\n",
    "    return test_scores, test_loss_av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss on the validation dataset is 809.5874714189712\n"
     ]
    }
   ],
   "source": [
    "(val_scores, val_loss_av) = train(num_epochs=1, val=True)\n",
    "print(f\"Mean loss on the validation dataset is {val_loss_av}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss on the test dataset is 927.167829506533\n"
     ]
    }
   ],
   "source": [
    "(test_scores, test_loss_av) = train(num_epochs=1, val=False)\n",
    "print(f\"Mean loss on the test dataset is {test_loss_av}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the default hyperparameters by validation on the 2014 dataset using the gaussian process optimisation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(low=1e-3, high=1e7, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_reg = Real(low=1e-6, high=1e3, prior='log-uniform',\n",
    "               name='reg')\n",
    "dim_alpha = Real(0, 1, name='alpha')\n",
    "dimensions = [dim_learning_rate, dim_reg, dim_alpha]\n",
    "\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def op_acc(learning_rate, reg, alpha):\n",
    "#     print(learning_rate, reg, alpha)\n",
    "    _, av_loss = train(val=True, learning_rate=learning_rate, reg=np.float32(reg),\n",
    "                       alpha=np.float32(alpha))\n",
    "    return av_loss\n",
    "\n",
    "\n",
    "search_result = gp_minimize(func=op_acc, dimensions=dimensions, n_calls=20, n_random_starts=10,\n",
    "                            verbose=True)\n",
    "print(search_result.x)\n",
    "[learning_rate, reg, alpha] = search_result.x\n",
    "\n",
    "(test_scores, test_av_loss) = train(num_epochs=3, val=False, learning_rate=learning_rate,\n",
    "                                    reg=np.float32(reg), alpha=np.float32(alpha))\n",
    "print(f\"\\nMean loss on the testing data is {test_av_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the hyperparameter search:\n",
    "\n",
    "Mean loss on the validation data is: 807\n",
    "\n",
    "Mean loss on the testing data is:    888"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
